# Project Agents.md Guide for OpenAI Codex

This Agents.md file provides comprehensive guidance for OpenAI Codex and other AI agents working with this codebase.

## Project Structure for OpenAI Codex Navigation

- `/src/components`: Core processing modules  
  - `image_uploader.py`: Handles receipt uploads  
  - `ocr_handler.py`: Extracts raw text from images  

- `/src/dbs`: Database and persistence layer  
  - `image_metadata.py`: Deduplication and image metadata  
  - `main_db.py`: Parsed items with taxonomy, qty, price  
  - `corrections_db.py`: User overrides for taxonomy  
  - `taxonomy_db.py`: Controlled taxonomy loader  
  - `faiss_store.py`: Vector storage & search  

- `/src/agents`: AI agents for receipt handling  
  - `parser.py`: OCR â†’ structured JSON (`{item, qty, price}`)  
  - `classifier.py`: Item â†’ taxonomy classification  

- `/src/llm`: LLM interaction layer  
  - `openai_client.py`: Wrapper for OpenAI/local LLM calls  

- `/src/integration`: External connectors  
  - `gsheet_handler.py`: Exports results to Google Sheets  

- `/src/utils`: Utility helpers  
  - `prompt_builder.py`, `validators.py`, `fingerprint.py`, `config.py`, `logger.py`  

- `/src/background.py`: Re-embedding + FAISS rebuild tasks  

- `/src/datamodels.py`: Data models (`ReceiptImage`, `ParsedItem`)  

- `/pages`: Streamlit UI pages  
  - `page1_upload.py`, `page2_review.py`, `page3_dashboard.py`  

---

## Coding Conventions for OpenAI Codex

### General Conventions
- Use **Python 3.10+** for all code generated by Codex  
- Follow **PEP8** and projectâ€™s existing style  
- Meaningful function/variable names required  
- Add **docstrings** for functions with complex logic  
- Use **type hints** consistently (`str`, `int`, `List[...]`)  

### Database & Persistence Guidelines
- Use **SQLite (via sqlite3)** consistently across DB modules  
- Do **not bypass corrections_db** when classifying items  
- Always maintain consistency between DB rows and FAISS indexes  

### Agents Guidelines
- `parser.py`: Must produce well-structured `ParsedItem` objects  
- `classifier.py`: Must validate taxonomy against controlled list  
- Agents must call `logger` for traceability of key actions  

### Utilities
- Always use `prompt_builder` for LLM prompts (no ad-hoc strings)  
- Use `validators` before writing taxonomy results to DB  

---

## Testing Requirements for OpenAI Codex

Run tests with:

```bash
# Run all tests
pytest

# Run specific test file
pytest tests/test_classifier.py

# Coverage check
pytest --cov=src
```

- Codex must maintain and extend test coverage when adding new features  
- Use `pytest` fixtures for DB setup/teardown  

---

## Pull Request Guidelines for OpenAI Codex

When Codex helps create a PR, ensure it:

1. Describes changes clearly (parser/classifier logic, DB changes, etc.)  
2. References any related issues (bug fixes, new features)  
3. Includes new/updated tests for any new code  
4. Ensures **FAISS indexes + DB schema stay consistent**  
5. Keeps PR focused on a single concern  

---

## Programmatic Checks for OpenAI Codex

Before merging code generated by Codex, run:

```bash
# Lint check
flake8 src/

# Type check
mypy src/

# Test & Coverage
pytest --cov=src

# Build/Run (Streamlit UI)
streamlit run pages/page1_upload.py
```

All checks must pass before merging Codex-generated code.

---

ðŸ“Œ **Summary for Codex**:  
Always follow the structured flow:  
*Upload â†’ OCR â†’ Parse â†’ Classify â†’ Save â†’ Review â†’ Correct â†’ Sync â†’ Report*  

Codex should enforce this pipeline, respect user corrections, and keep embeddings/databases consistent.


# Repository Guidelines

## Project Structure & Module Organization
The application code lives in `src/`. Use `src/agents/` for agent orchestration (classifier, parser), `src/components/` for UI/logic building blocks, `src/dbs/` for database adapters, `src/llm/` for model integrations, `src/utils/` for shared helpers, plus `logger.py` and `exception.py` for observability and error handling. Streamlit page flows are under `pages/` (`page1_upload.py` etc.). `data/` stores SQLite and index artifacts; avoid editing them manually. `notebooks/` holds exploratory work, and `tests/` houses pytest suites. `main.py` wires the CLI entry point; `project_structure.py` prints the folder layout for quick reference.

## Build, Test, and Development Commands
- `uv sync` installs Python 3.10 dependencies from `pyproject.toml`/`uv.lock`.
- `uv run python main.py` executes the CLI agent pipeline end to end.
- `uv run streamlit run pages/page1_upload.py` launches the multi-step UI for local review.
- `uv run pytest` runs the entire automated test suite; use `uv run pytest tests/test_end_to_end.py` when debugging the pipeline.

## Coding Style & Naming Conventions
Follow PEP 8 with 4-space indentation. Use `snake_case` for functions and modules, `PascalCase` for classes, and descriptive names for agents (e.g., `InvoiceClassifier`). Keep reusable logic in `src/utils/` and isolate I/O inside `src/dbs/` or `pages/`. Include docstrings summarizing agent responsibilities and prefer dataclasses for structured payloads.

## Testing Guidelines
Write pytest tests in `tests/` with the `test_*.py` pattern and descriptive method names. Unit tests should target individual agents or components, while integration coverage extends the full pipeline (`test_end_to_end.py`). When adding a feature, provide at least one regression test and update fixtures to match any schema changes. Run `uv run pytest --maxfail=1 --disable-warnings` before opening a PR.

## Commit & Pull Request Guidelines
Craft commits in present-tense imperative (`Add receipt parser logging`) and keep them scoped to one concern. Reference Jira/Ticket IDs in the body when available. Pull requests must outline the change motivation, highlight affected agents/components, link related issues, and attach screenshots or console output for UI or pipeline updates. Request reviews from domain owners of touched modules and ensure CI lights stay green before merging.

## Agent-Specific Notes
Document new agents in `src/agents/__init__.py`, include a stub registration helper, and validate them with both unit and end-to-end tests. Share sample payloads in the PR body so reviewers can reproduce local runs.
