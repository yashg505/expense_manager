# Expense Manager

An intelligent expense management system that leverages OCR (RapidOCR/PaddleOCR) and LLMs (OpenAI/Gemini) to automate the extraction, classification, and organization of receipt data.

## ğŸš€ Overview

Expense Manager transforms messy receipt images into structured data. It uses advanced OCR to read text, employs Large Language Models to interpret and extract line items, and applies a vector-search-based taxonomy system to classify expenses accurately. Finally, it exports the processed data to Google Sheets for easy tracking.

### Key Features
- **Intelligent OCR**: Uses RapidOCR and PaddleOCR for high-accuracy text extraction from images.
- **LLM-Powered Parsing**: Extracts structured receipt data (vendor, date, total, line items) using GPT or Gemini .
- **Smart Classification**: Automatically assigns categories to items based on a taxonomy stored in PostgreSQL (using `pgvector` for native semantic search).
- **Deduplication**: Uses image fingerprinting (ImageHash) to prevent duplicate receipt uploads.
- **Streamlit UI**: A user-friendly, multi-page web interface for uploading, reviewing, and confirming expenses.
- **Google Sheets Integration**: Seamlessly syncs confirmed expenses to a centralized spreadsheet.
- **CI/CD Pipeline**: Automated testing and deployment to Google Cloud (GCE) using GitHub Actions and Docker.

## ğŸ›  Technology Stack
- **Frontend**: [Streamlit](https://streamlit.io/)
- **OCR**: [RapidOCR](https://github.com/RapidAI/RapidOCR), [PaddleOCR](https://github.com/PaddlePaddle/PaddleOCR)
- **LLM**: OpenAI, Google Gemini
- **Data Validation**: [Pydantic](https://docs.pydantic.dev/)
- **Database**: PostgreSQL with `pgvector` (Vector Search), SQLite (Metadata)
- **Embeddings**: Sentence-Transformers (`all-MiniLM-L6-v2`)
- **Dependency Management**: [uv](https://github.com/astral-sh/uv)
- **Cloud**: Google Cloud Platform (GCS, Secret Manager, Artifact Registry, GCE)

## ğŸ“ Project Structure
```text
â”œâ”€â”€ src/expense_manager/ # Core logic
â”‚   â”œâ”€â”€ agents/          # LLM Parsing and Classification logic
â”‚   â”œâ”€â”€ components/      # UI components (uploader, navbar, etc.)
â”‚   â”œâ”€â”€ dbs/             # Database handlers (PostgreSQL, SQLite)
â”‚   â”œâ”€â”€ integration/     # Google Sheets handler
â”‚   â”œâ”€â”€ llm/             # OpenAI and Gemini client wrappers
â”‚   â”œâ”€â”€ models/          # Pydantic data models
â”‚   â”œâ”€â”€ sync/            # Taxonomy synchronization logic
â”‚   â””â”€â”€ utils/           # Logging, image fingerprinting, embeddings, etc.
â”œâ”€â”€ pages/               # Streamlit multi-page application logic
â”œâ”€â”€ scripts/             # Utility scripts (sync, setup, deployment)
â”œâ”€â”€ data/                # SQLite databases and taxonomy files
â”œâ”€â”€ artifacts/           # Uploaded images and metadata
â”œâ”€â”€ tests/               # Pytest suite
â”œâ”€â”€ .github/workflows/   # CI/CD Pipeline
â””â”€â”€ Dockerfile           # Containerization setup
```

## âš™ï¸ Setup & Installation

### Prerequisites
- Python 3.10+
- [uv](https://github.com/astral-sh/uv) package manager
- PostgreSQL database with `pgvector` extension enabled.

### Installation
1. Clone the repository.
2. Install dependencies using `uv`:
   ```bash
   uv sync
   ```

### Configuration
1. Create a `.env` file in the root directory and add your API keys:
   ```env
   OPENAI_API_KEY=your_openai_key
   GEMINI_API_KEY=your_gemini_key
   NEON_CONN_STR=postgresql://user:password@host/dbname
   ```
2. Update `config.yaml` with your specific `sheet_id` and other preferences.

## ğŸ’» Usage

### Running the Streamlit UI
```bash
uv run streamlit run main.py
```
This will launch the application in your browser:
1. **Upload**: Drop receipt images (detects duplicates automatically).
2. **Review**: Check extracted data and adjust classifications.
3. **Confirm**: Finalize and sync data to Google Sheets.

### Syncing Taxonomy
To sync the taxonomy from Google Sheets to the PostgreSQL database and update the vector embeddings:
```bash
uv run python scripts/build_taxonomy_index.py
```

## ğŸ§ª Testing
Run the test suite using `pytest`:
```bash
uv run pytest
```

## ğŸš¢ Deployment
The project is containerized using Docker and deployed via GitHub Actions.

- **Build Image**: `docker build -t expense-manager .`
- **Run Locally**: `docker run -p 8501:8501 --env-file .env expense-manager`
- **CI/CD**: Pushing to the `master` branch triggers the `.github/workflows/pipeline.yml`, which tests, builds, and deploys the app to Google Compute Engine.

## ğŸ“ Development Conventions
- **Style**: Follow [PEP 8](https://peps.python.org/pep-0008/).
- **Typing**: Use Python type hints consistently.
- **Logging**: Use the internal `expense_manager.logger` for tracing.
- **Errors**: Use `expense_manager.exception.CustomException` for consistent error handling.
